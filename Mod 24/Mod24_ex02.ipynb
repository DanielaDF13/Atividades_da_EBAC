{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cite 5 diferenças entre o AdaBoost e o GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e6f7ff; padding: 20px; border: 3px solid #00b3b3; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\">\n",
    "   <strong>Resposta:</strong> <br>\n",
    "   <strong>1.</strong> O AdaBoost possui uma floresta de Stumps e o GBM uma floresta de árvores; <br>\n",
    "   <strong>2.</strong> No AdaBoost primeiro passo é um stump e no GBM é uma média do Y; <br>\n",
    "   <strong>3.</strong> No AdaBoost cada resposta tem um peso diferente e no GBM todas as respostas das árvores possui um multiplicador em comum chamado lerning_rate;  <br>\n",
    "   <strong>4.</strong>AdaBoost tende a ser mais rápido para treinar por utilizar árvores rasas e não precisa calcular gradientes complexos;  <br>\n",
    "   <strong>5.</strong> AdaBoost é mais sensível a outliers do que GBM, pois o AdaBoost dá mais peso às amostras que são mal classificadas. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Acesse o link Scikit-learn– GBM, leia a explicação(traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cite 5 Hyperparametros importantes no GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e6f7ff; padding: 20px; border: 3px solid #00b3b3; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\">\n",
    "    <strong> subsample: </strong> Determina o tamanho da amostra aleatória usada para treinar cada árvore. O valor se encontra entre 0 e 1, se o valor for inferior a 1 se trata de um Stochastic Gradient Boosting. <br>\n",
    "    <strong> n_estimators: </strong> Controla o número de iterações do algoritmo.<br>\n",
    "    <strong> learning_rate: </strong> Ajusta a magnitude da correção aplicada ao modelo a cada iteração.<br>\n",
    "    <strong> min_samples_split: </strong> Controla o número mínimo de amostras necessárias para dividir um nó em uma árvore de decisão. <br>\n",
    "    <strong> max_depth: </strong> Controla a profundidade máxima das árvores de decisão <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. (Opcional) Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'criterion': 'friedman_mse', 'learning_rate': 0.5, 'max_depth': 1, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Definindo o modelo\n",
    "clf = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "# Definindo o grid de parâmetros a ser testado\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],          # Número de árvores no modelo\n",
    "    'learning_rate': [0.1, 0.5],        # Taxa de aprendizado\n",
    "    'max_depth': [1, 3],                # Profundidade máxima das árvores\n",
    "    'subsample': [0.8, 1.0],            # Fração de amostras para cada árvore\n",
    "    'min_samples_split': [2, 5],        # Número mínimo de amostras para dividir um nó\n",
    "    'criterion': ['friedman_mse', 'mse', 'mae']  # Função de avaliação da divisão das árvores\n",
    "}\n",
    "\n",
    "# Configurando o HalvingGridSearchCV\n",
    "halving_grid_search = HalvingGridSearchCV(estimator=clf, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=2, factor=2, min_resources='exhaust')\n",
    "\n",
    "# Treinando o modelo com o grid de parâmetros\n",
    "halving_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Exibindo os melhores parâmetros encontrados\n",
    "print(\"Melhores parâmetros:\", halving_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Acessando o artigo do Jerome Friedman (Stochastic ) e pensando no nome dado ao Stochastic GBM, qual maior diferença entre os dois algoritimos?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e6f7ff; padding: 20px; border: 3px solid #00b3b3; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\">\n",
    "   <strong>Resposta:</strong> A maior diferença entre os dois algoritmos é que o Stochastic GBM utiliza uma amostra aleatória em cada iteração para treinar a árvore de dados.  \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
