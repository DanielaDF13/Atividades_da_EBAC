{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba19abf-1a86-44d4-b2a3-476ca4324327",
   "metadata": {},
   "source": [
    "## 1. Quais são os hyperparâmetros do RF?\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d67efc-4884-4ced-b4f7-2ab0cb344b02",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e6f7ff; padding: 20px; border: 3px solid #00b3b3; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\">\n",
    "   <strong>Resposta:</strong>\n",
    "    n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, min_impurity_decrease, bootstrap, oob_score, n_jobs, random_state, verbose, warm_start, class_weight, ccp_alpha, max_samples e  monotonic_cst\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a9f3a6-7bbe-4a6e-9793-fe0cd9db7822",
   "metadata": {},
   "source": [
    "## 2. Pra que serve cada um deles?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557c5303-6714-4bdb-a195-8deca2853e2e",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #e6f7ff; padding: 20px; border: 3px solid #00b3b3; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\">\n",
    "    <strong>n_estimators:</strong> Se refere ao número de árvores de decisão que serão construídas no modelo. Aumentar o número de árvores aumenta a performance do modelo porém também aumenta o custo computacional. <br>\n",
    "    <strong>criterion:</strong> Controla o método utilizado para medir a qualidade das divisões nas árvores de decisão dentro do Random Forest. As opções são \"gini\" , \"entropy\" e “log_loss”. <br>\n",
    "    <strong>max_depth:</strong> A profundidade máxima das árvores de decisão,limitando o número de divisões que uma árvore pode ter. Um valor menor ajuda a evitar o overfitting, mantendo o modelo mais simples, enquanto um valor maior pode levar a modelos mais complexos, com maior risco de overfitting. <br>\n",
    "    <strong> min_samples_split:</strong> Define o número mínimo de amostras necessárias para dividir um nó. Um valor maior restringe as divisões e pode ajudar a reduzir o overfitting, enquanto um valor muito pequeno permite divisões mais profundas, o que pode aumentar o risco de overfitting. <br>\n",
    "    <strong>min_samples_leaf:</strong> Controla o número mínimo de amostras necessárias para que um nó seja considerado uma folha. Um valor maior ajuda a evitar o overfitting, forçando a árvore a criar folhas mais gerais, enquanto um valor muito baixo pode permitir divisões muito específicas, aumentando o risco de overfitting. <br>\n",
    "    <strong>min_weight_fraction_leaf:</strong> Controla a fração mínima do peso total das amostras que um nó folha deve conter. Ele é útil quando as amostras têm pesos desiguais e ajuda a evitar que a árvore crie folhas pequenas e detalhadas, reduzindo o risco de overfitting.\n",
    "<br>\n",
    "    <strong>max_features :</strong> É o hiperparâmetro que controla o número m de colunas utilizados no feature selection, onde por padrão m é para classificação a raiz de p e para regressão um terço de p onde p é o número de colunas do dataset original, utilizando um valor alto de m as árvores serão mais semelhantes entre si podendo resultar em overfitting, como um valor baixo de m se tem árvores mais diversas e reduz o overfitting. No entanto, se o valor for muito baixo, pode causar underfitting. <br>\n",
    "    <strong>max_leaf_nodes :</strong> Controla o número máximo de nós folha que uma árvore de decisão pode ter.um valor alto permite mais folhas aumentando a complexidade da árvore, o que pode melhorar a precisão do modelo, porém também pode aumentar o risco de overfitting, Limitar o número de folhas pode resultar em uma árvore mais simples e com menos capacidade de capturar detalhes complexos dos dados. Isso pode ajudar a reduzir o overfitting, mas também pode causar underfitting. <br>\n",
    "   <strong>min_impurity_decrease: </strong> Define a redução mínima na impureza que deve ser alcançada para que uma nova divisão seja feita em um nó da árvore. Se o valor for alto o modelo será mais restritivo e fará menos divisões, resultando em árvores mais simples e com menor risco de overfitting. Porém, isso também pode levar a underfitting, se o valor for baixo o modelo permitirá mais divisões, mesmo que a redução na impureza seja pequena. Isso pode resultar em árvores mais profundas e mais complexas, o que aumenta o risco de overfitting.  <br>\n",
    "    <strong>bootstarp :</strong>Determina se o treinamento das árvores será feito com amostras com reposição (quando bootstrap=True) ou sem reposição (quando bootstrap=False). A configuração padrão é True, o que favorece a criação de árvores diversificadas e a redução do risco de overfitting, contribuindo para a robustez e a capacidade de generalização do modelo.  <br>\n",
    "    <strong>oob_score :</strong>Permite calcular a pontuação de out-of-bag (OOB), uma técnica de validação interna que utiliza as amostras que não foram usadas para treinar uma árvore específica. Com oob_score=True, o modelo estima sua capacidade de generalização sem a necessidade de um conjunto de validação separado. Isso oferece uma forma eficaz de avaliar o modelo e ajuda a evitar o overfitting.  <br>\n",
    "    <strong>n_jobs :</strong> Permite controlar o número de núcleos de CPU a serem usados para paralelizar o treinamento e a previsão do modelo. Definir n_jobs=-1 aproveita todos os núcleos disponíveis para otimizar o tempo de execução, enquanto valores menores de n_jobs (como n_jobs=1) resultam em uma execução sequencial mais lenta. Esse parâmetro é útil para otimizar o desempenho do modelo, especialmente quando lidamos com grandes volumes de dados.  <br>\n",
    "    <strong>random_state :</strong>Controla a sequência de aleatoriedade durante o treinamento do modelo, garantindo reprodutibilidade dos resultados. <br>\n",
    "    <strong>verbose :</strong>Controla o nível de informações detalhadas que são exibidas durante o treinamento. Um valor de verbose=0 (padrão) suprime qualquer mensagem, enquanto verbose=1 e verbose=2 fornecem informações progressivas, desde um monitoramento básico até um detalhamento completo das operações. Ele é útil para acompanhar o progresso do treinamento e para depuração, especialmente ao lidar com modelos grandes ou complexos. <br>\n",
    "    <strong>warm_start :</strong>Permite que você continue o treinamento de um modelo previamente treinado, sem precisar recomeçar o processo do início. Quando definido como True, ele permite adicionar mais árvores ao modelo existente, o que pode economizar tempo de treinamento quando o número de árvores (n_estimators) é ajustado após o treinamento inicial. Ele é útil para ajustes incrementais e pode acelerar o processo de treinamento quando se busca melhorar o modelo sem recomeçar do zero.  <br>\n",
    "    <strong>class_weight :</strong>Permite ajustar a importância relativa das classes no processo de treinamento, ajudando a lidar com problemas de desbalanceamento de classes. Quando definido como 'balanced', ele ajusta automaticamente os pesos com base na frequência das classes. Além disso, você pode especificar pesos personalizados para cada classe, de acordo com a importância que você deseja atribuir a elas. Esse parâmetro é essencial para melhorar o desempenho do modelo em problemas onde algumas classes são muito menos representadas do que outras.  <br>\n",
    "    <strong>ccp_alpha :</strong>É utilizado para controlar o custo da poda das árvores de decisão durante o treinamento, aplicando uma técnica conhecida como poda de custo-complexidade (ou cost-complexity pruning). Com ccp_alpha=0 não há poda, à medida que você aumenta o valor de ccp_alpha, as árvores serão mais podadas, removendo nós e ramos menos significativos, resultando em árvores mais simples e com menor risco de overfitting. No entanto, valores muito altos podem levar a uma perda excessiva de informações, prejudicando a performance do modelo. \n",
    " <br>\n",
    "   <strong>max_samples :</strong>Controla o número de amostras usadas para treinar cada árvore individualmente. Ele pode ser definido como uma fração ou como um número fixo de amostras. Esse parâmetro ajuda a aumentar a diversidade entre as árvores, o que pode melhorar a generalização do modelo e reduzir o overfitting. Além disso, pode aumentar a eficiência de treinamento, pois cada árvore é treinada com um subconjunto menor de dados. No entanto, valores muito baixos de max_samples podem reduzir a precisão do modelo. <br>\n",
    "   <strong> monotonic_cst :</strong> serve para impor restrições de monotonicidade no modelo, garantindo que a previsão do modelo siga um padrão monotônico (crescente ou decrescente) com relação a certas variáveis de entrada. Isso é útil quando você tem conhecimento prévio sobre a relação entre as variáveis e a variável alvo, ou quando você deseja garantir que o modelo se comporte de maneira mais consistente com as expectativas do domínio.\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
